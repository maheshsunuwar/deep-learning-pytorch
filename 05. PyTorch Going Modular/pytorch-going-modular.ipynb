{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8b5a627-8e13-4d6f-88df-2fa92d6ab1ab",
   "metadata": {},
   "source": [
    "# 05. PyTorch Going Modular\n",
    "\n",
    "This section is all about turning jupyter notebook to python scripts\n",
    "\n",
    "`Jupyter Notebook -> Python Scripts`\n",
    "\n",
    "Going Modular is turning Notebook to series of different Python scripts that offer similar functionality.\n",
    "\n",
    "for example, we can turn our notebook code into following Python files\"\n",
    "- `data_setup.py` - a file to prepare and download data if needed\n",
    "- `engine.py` - a file containing various `training` functions\n",
    "- `model_builder.py` - a file to create a PyTorch model\n",
    "- `train.py` - a file to leverage all other files and train a target PyTorch model\n",
    "- `utils.py` - a file dedicated to helpful utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e205946-98ad-4bf4-9196-e211a1be0082",
   "metadata": {},
   "source": [
    "libraries like fast.ai's `nb-dev` enables us to write whole Python Library with Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9820dc-6f71-4123-85a8-da72ad045369",
   "metadata": {},
   "source": [
    "Workflow:\n",
    "1. Start with Jupyter notebook for a quick experiment and visualization\n",
    "2. when something's working, move the most useful code to Python script.\n",
    "\n",
    "## What we are going to cover\n",
    "1. Going Modular: Part 1 (cell mode)\n",
    "2. Going Modular: Part 2 (script mode)\n",
    "\n",
    "We will work towards:\n",
    "\n",
    "Converting our notebook into the files in following folder structure\n",
    "```\n",
    "going_modular/\n",
    "├── going_modular/\n",
    "│   ├── data_setup.py\n",
    "│   ├── engine.py\n",
    "│   ├── model_builder.py\n",
    "│   ├── train.py\n",
    "│   └── utils.py\n",
    "├── models/\n",
    "│   ├── 05_going_modular_cell_mode_tinyvgg_model.pth\n",
    "│   └── 05_going_modular_script_mode_tinyvgg_model.pth\n",
    "└── data/\n",
    "    └── pizza_steak_sushi/\n",
    "        ├── train/\n",
    "        │   ├── pizza/\n",
    "        │   │   ├── image01.jpeg\n",
    "        │   │   └── ...\n",
    "        │   ├── steak/\n",
    "        │   └── sushi/\n",
    "        └── test/\n",
    "            ├── pizza/\n",
    "            ├── steak/\n",
    "            └── sushi/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43675ad-8314-4e25-bd97-af8ed4997cb3",
   "metadata": {},
   "source": [
    "## 1. Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ef292fe-34f0-4357-aa0b-dda013d183c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/pizza_steak_sushi already exists\n",
      "downloading...\n",
      "Done.\n",
      "extracting...\n",
      "Extraction done\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "from zipfile import ZipFile\n",
    "import os\n",
    "\n",
    "data_path = Path('data')\n",
    "image_data_path = data_path/ \"pizza_steak_sushi\"\n",
    "\n",
    "if image_data_path.is_dir():\n",
    "    print(f'{image_data_path} already exists')\n",
    "else:\n",
    "    print(f'Creating folders: {image_data_path}')\n",
    "    image_data_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "data_url = 'https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip'\n",
    "zip_file_name = 'pizza_steak_sushi.zip'\n",
    "with open(image_data_path/zip_file_name, 'wb') as f:\n",
    "    print('downloading...')\n",
    "    request = requests.get(data_url)\n",
    "    f.write(request.content)\n",
    "    print(f'Done.')\n",
    "\n",
    "with ZipFile(image_data_path/zip_file_name, 'r') as zip_file:\n",
    "    print('extracting...')\n",
    "    zip_file.extractall(image_data_path)\n",
    "    print('Extraction done')\n",
    "\n",
    "# remove zip file\n",
    "os.remove(image_data_path/zip_file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9f82e3-c46a-4160-a2fc-de49b21a40ce",
   "metadata": {},
   "source": [
    "Now we have this structure:\n",
    "\n",
    "```\n",
    "data/\n",
    "└── pizza_steak_sushi/\n",
    "    ├── train/\n",
    "    │   ├── pizza/\n",
    "    │   │   ├── train_image01.jpeg\n",
    "    │   │   ├── test_image02.jpeg\n",
    "    │   │   └── ...\n",
    "    │   ├── steak/\n",
    "    │   │   └── ...\n",
    "    │   └── sushi/\n",
    "    │       └── ...\n",
    "    └── test/\n",
    "        ├── pizza/\n",
    "        │   ├── test_image01.jpeg\n",
    "        │   └── test_image02.jpeg\n",
    "        ├── steak/\n",
    "        └── sushi/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b680b5a-1802-4571-a872-fd0ba0589d97",
   "metadata": {},
   "source": [
    "## 2.Create datasets and Dataloaders `data_setup.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d88cafac-99fe-4103-a566-c2d59f0f0546",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6547b53c-cd33-4226-a662-3afca4cbed4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "def create_dataloaders(\n",
    "    train_dir: str,\n",
    "    test_dir: str,\n",
    "    transform: transforms.Compose,\n",
    "    batch_size: int,\n",
    "    num_workers: int = NUM_WORKERS\n",
    "):\n",
    "    \"\"\"Creates training and testing DataLoaders\n",
    "    Args:\n",
    "        train_dir: Path to the training directory\n",
    "        test_dir: Path to the testing directory\n",
    "        transform: torchvision transforms to perform on training and testing data\n",
    "        batch_size: Number of samples per batch in each of the DataLoaders\n",
    "        num_workers: An integer for number of workers per DataLoader\n",
    "\n",
    "    Returns:\n",
    "        A tuple of (train_dataloader, test_dataloader, class_names)\n",
    "    \"\"\"\n",
    "    train_data = datasets.ImageFolder(train_dir, transform=transform)\n",
    "    test_data = datasets.ImageFolder(test_dir, transform=transform)\n",
    "\n",
    "    # get class names\n",
    "    class_names = train_data.class_names\n",
    "\n",
    "    # data loaders\n",
    "    train_dataloader = DataLoader(dataset=train_data,\n",
    "                                  batch_size=batch_size,\n",
    "                                  num_workers = num_workers,\n",
    "                                  shuffle=True)\n",
    "    test_dataloader = DataLoader(dataset=test_data,\n",
    "                                  batch_size=batch_size,\n",
    "                                  num_workers = num_workers,\n",
    "                                  shuffle=False)\n",
    "    return train_dataloader, test_dataloader, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c145e5e7-38f8-4b86-ad23-ad0ba12ac3fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
