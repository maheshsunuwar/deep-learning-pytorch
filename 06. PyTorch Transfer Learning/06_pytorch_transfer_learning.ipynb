{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb4fed06-1492-4d43-adaa-f7e3fe3c9af2",
   "metadata": {},
   "source": [
    "# 06. PyTorch Transfer Learning\n",
    "\n",
    "Let's take a well performing pre-trained model and adjust it to one of our own problems.\n",
    "\n",
    "We can increase the performance of our model by getting a help from other already trained good models.\n",
    "\n",
    "**What we're going to cover**\n",
    "- take a pretrained model from `torchvision.models`\n",
    "- and customize it to work on (and hopefully improve) our `foodvision` problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776178de-f20a-479e-a498-6cdae42b4058",
   "metadata": {},
   "source": [
    "Steps that we will do:\n",
    "1. Setting up\n",
    "2. Get data\n",
    "3. Create datasets ad dataloaders\n",
    "4. Get and Customize a pretrained model\n",
    "5. Train model\n",
    "6. Evaluate the model by plotting loss curves\n",
    "7. Make predictions on images from the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47eb8b52-829b-4b77-9098-8a8b79083986",
   "metadata": {},
   "source": [
    "## 1. Setting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c69dbbda-95b5-4a76-a1e1-87a1caa1e75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.2.2\n",
      "torchvision version: 0.17.2\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import torch\n",
    "    import torchvision\n",
    "    assert int(torch.__version__.split('.')[0]) >=2, 'torch version should be 2.0+'\n",
    "    assert int(torchvision.__version__.split('.')[1]) >=17, 'torchvision version should be 0.17+'\n",
    "    print(f'torch version: {torch.__version__}')\n",
    "    print(f'torchvision version: {torchvision.__version__}')\n",
    "except:\n",
    "    print('[INFO] torch/torchvision verson not as required, installing nightly versions')\n",
    "    !pip install -U torch torchvision\n",
    "    import torch\n",
    "    import torchvision\n",
    "    print(f'torch version: {torch.__version__}')\n",
    "    print(f'torchvision version: {torchvision.__version__}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dec8b444-eb72-41f2-90ab-909cac79d505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plot\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "\n",
    "# try torchinfo, install if not installed\n",
    "try:\n",
    "    from torchinfo import summary\n",
    "except:\n",
    "    print(f\"[INFO] Couldn't find torchinfo, intalling it.\")\n",
    "    !pip install -q torchinfo\n",
    "    print(f'installed torchinfo {torchinfo.__version__}')\n",
    "    from torchinfo import summary\n",
    "\n",
    "## setup device agnostic code\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078c5a8e-e7a5-4fbe-9fcb-53cd49db4cb8",
   "metadata": {},
   "source": [
    "## 2. Getting Data\n",
    "\n",
    "before we start transfer learning, we need a dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9fe4f6b3-2a1c-4641-b018-1e2fdda341b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folders doesn't exist, creating one..\n",
      "Downloading data...\n",
      "Download done.\n",
      "Unzipping...\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from zipfile import ZipFile\n",
    "import requests\n",
    "import os\n",
    "\n",
    "data_url = \"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\"\n",
    "\n",
    "data_path = Path('data')\n",
    "image_data_path = data_path / 'pizza_steak_sushi'\n",
    "\n",
    "# create folders if not already available\n",
    "if image_data_path.is_dir():\n",
    "    print('folders already eists')\n",
    "else:\n",
    "    print(\"Folders doesn't exist, creating one..\")\n",
    "    image_data_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # download data\n",
    "    with open(data_path / 'pizza_steak_sushi.zip', 'wb') as f:\n",
    "        request = requests.get(data_url)\n",
    "        print(f'Downloading data...')\n",
    "        f.write(request.content)\n",
    "        print(f'Download done.')\n",
    "    # unzip\n",
    "    with ZipFile(data_path/'pizza_steak_sushi.zip', 'r') as zip_file:\n",
    "        print(f'Unzipping...')\n",
    "        zip_file.extractall(image_data_path)\n",
    "        print('Extracted..')\n",
    "    # remove zip file\n",
    "    os.remove(data_path/'pizza_steak_sushi.zip')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849e2143-b4da-4f90-8050-f51ddcfcbc2f",
   "metadata": {},
   "source": [
    "## 3. Create datasets and dataloader\n",
    "now that we have downloaded the data we should create `datasets` and corresponding `dataloader`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2825a5c8-228f-42d9-9ef7-078dc43f93d8",
   "metadata": {},
   "source": [
    "### 3.1 Creating a transform for torchvision.models (manual = old method)\n",
    "\n",
    "since we'll be using a pretrained model from `torchvision.models`, there's a specific transform we need to prepare our images first.\n",
    "\n",
    "When using a pretrained model, it's important that **your custom data going into the model is prepared in the same way as the original training data that went into the model**\n",
    "\n",
    "1. All pretrained models expect input images normalized in the same way, i.e. `mini batches` of 3-channel images of shape (3 x H x W) where H and W are expected to be at least 224.\n",
    "    - But some models might expect H and W to be of other sizes. We have to check the documentation for that.\n",
    "2. Images have to be loaded in to a range of [0,1[\n",
    "3. then normalized using `mean = [0.485, 0.456, 0.406]` and `std = [0.229, 0.224, 0.225]`\n",
    "\n",
    "we can achieve this in the following way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e98afcc7-27de-47c2-b623-d7c51376e18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_transform = transforms.Compose([\n",
    "    transforms.Resize(size=(224,224)), # mini batchs of shape 3,224,224\n",
    "    transforms.ToTensor(), # turn image values to between 0, 1\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std = [0.229, 0.224, 0.225]\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c4b0de96-0393-4b77-8c49-396bb2d26bbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PosixPath('data/pizza_steak_sushi/train'),\n",
       " PosixPath('data/pizza_steak_sushi/test'))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_path = data_path / 'pizza_steak_sushi'/ 'train'\n",
    "test_data_path = data_path / 'pizza_steak_sushi'/ 'test'\n",
    "\n",
    "train_data_path, test_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "87863e59-bd6a-489e-b3c2-a033ad58afbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pizza', 'steak', 'sushi']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = torchvision.datasets.ImageFolder(root=train_data_path,\n",
    "                                              transform=manual_transform,\n",
    "                                              target_transform=None)\n",
    "test_data = torchvision.datasets.ImageFolder(root=test_data_path,\n",
    "                                              transform=manual_transform,\n",
    "                                              target_transform=None)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "train_dataloader = torch.utils.data.DataLoader(dataset=train_data,\n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               num_workers=NUM_WORKERS,\n",
    "                                               shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(dataset=test_data,\n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               num_workers=NUM_WORKERS,\n",
    "                                               shuffle=False)\n",
    "class_names = train_data.classes\n",
    "class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d882b9-b4ac-45af-b008-5b6a56f8a30d",
   "metadata": {},
   "source": [
    "### 3.2 Creating a transform for `torchvision.models` - new method\n",
    "\n",
    "As of `torchvision v0.13+`, an automatic transform creation feature has been added.\n",
    "\n",
    "When you setup a model from `torchvision.models` \n",
    "\n",
    "select a pretrained model weights you'd like to use\n",
    "\n",
    "for example:\n",
    "\n",
    "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n",
    "\n",
    "where,\n",
    "- `EfficientNet_B0_Weights` is the model architecture weights we'd like to use\n",
    "- `DEFAULT` means the best available weights (the best performance in ImageNet)\n",
    "  - Depending on the model architecture we can also see 'IMAGENET_V1` and `IMAGENET_V2`, where generally the higher version number is better.\n",
    "  - `DEFAULT` is easiest option to choose.\n",
    "\n",
    "let's try it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "037ad25c-5fcc-4eec-b16c-2113711770a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EfficientNet_B0_Weights.IMAGENET1K_V1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73e3223-4fc8-451e-b458-b2826c5e3f8d",
   "metadata": {},
   "source": [
    "Now we have access to the transform associated with the `EfficientNet_BO_Weights model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6785d785-d7ff-4be9-bb06-8d42e5a3c680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassification(\n",
       "    crop_size=[224]\n",
       "    resize_size=[256]\n",
       "    mean=[0.485, 0.456, 0.406]\n",
       "    std=[0.229, 0.224, 0.225]\n",
       "    interpolation=InterpolationMode.BICUBIC\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_transforms = weights.transforms()\n",
    "auto_transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd0788e-5c3a-45ba-816c-4fbfbc829157",
   "metadata": {},
   "source": [
    "The benefit of using auto transform is that, it `comes` with the `model architecture` we chose to use.\n",
    "\n",
    "we don't have to create it by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0350ccdc-6669-41bc-a271-760b379fbbd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
