{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2958910d-be87-4adc-80b8-6c5bc7ecb2d0",
   "metadata": {},
   "source": [
    "# 09. PyTorch Model Deployment\n",
    "\n",
    "Lets bring FoodVision to life and make it publicly accessible.\n",
    "\n",
    "**We are going to deploy our FoodVision model to the internet as a usable app!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d5caad-ce3f-40bf-b4a0-d15ae0997135",
   "metadata": {},
   "source": [
    "## What is machine learning model deployment?\n",
    "\n",
    "***Machine learning model deployment** is the process of making your machine learning model accessible for others.*\n",
    "\n",
    "For example, someone taking a photo on their smartphone of food and then having our FoodVision model classify it into pizza, steak, or sushi.\n",
    "\n",
    "Some other examples can be, *an operating system may lower its resource consumption based on a machine learning model making predictions on how much power someone generally uses at specific times of day.*\n",
    "\n",
    "Also these models can learn from each other as well. For example, a Tesla car's computer vision system will interact with the car's route planning program and then the route planning program will get inputs and feedback from the driver.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069eaf5e-2a9f-4efa-a405-1f652a799a74",
   "metadata": {},
   "source": [
    "## Why deploy a machine learning model?\n",
    "\n",
    "One of the most important philosophical question in machine learning is:\n",
    "***if a machine learning model never leaves a notebook, does it exist?***\n",
    "\n",
    "---\n",
    "Deploying a model is as important as training one.\n",
    "\n",
    "Because although you can get a pretty good idea of how your model is going to function by evaluating it on a well crafted test set or visualizing its results, you never really know how it will perform ultil you release it to the wild.\n",
    "\n",
    "***Having people who've never used your model interact with it will often reveal edge cases you never thought of during training.***\n",
    "\n",
    "For example, what happens if someone was to upload a photo that wasn't of food to our FoodVision model?\n",
    "\n",
    "One solution would be to create another model that firstclassifies images as \"food\" or \"not food\" and passing the target image through that model first.\n",
    "\n",
    "THen if the images is of \"food\" it goes to our FoodVision model and gets classifies into pizza, steak, or sushi.\n",
    "\n",
    "And if it's \"not food\", a message is displayed.\n",
    "\n",
    "But what is these predictions were wrong?\n",
    "\n",
    "What happens then?\n",
    "\n",
    "You can see how these questions could keep going.\n",
    "\n",
    "Thus this highlights the importance of model deployment: it helps you figure our errors in your model that aren't obvious during training/testing.\n",
    "\n",
    "---\n",
    "\n",
    "***But once you've got a good model, deployment is a good next step. Monitoring involves seeing how your model goes on the most important data split: data from the real world.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a600cb-b1de-4d92-8e6e-b76d399c43b9",
   "metadata": {},
   "source": [
    "## Different types of machine learning model deployment\n",
    "\n",
    "Whole books could be written on the different types of machine learning model deployment(and many good one are listed [PyTorch Extra Resources](https://www.learnpytorch.io/pytorch_extra_resources/#resources-for-machine-learning-and-deep-learning-engineering))\n",
    "\n",
    "---\n",
    "Let's start with a simple question:\n",
    "> What is the most ideal scenario for our machine leanring model to be used?\n",
    "\n",
    "ANd then work backward from there.\n",
    "\n",
    "In case of FoodVision, our ideal scenerio might be:\n",
    "- someone taking a photo on a mobile device(through an app or web browser)\n",
    "- The prediction comes back fast.\n",
    "\n",
    "Easy.\n",
    "\n",
    "So we have two main criteria:\n",
    "1. The model should work on a mobile device\n",
    "2. THe model should make predictions *fast*(because a slow app is a boring app).\n",
    "\n",
    "And of course, depending on our use case, our requirements may vary.\n",
    "\n",
    "We may notive the above two points break down into another two questions:\n",
    "1. **Where's it going to go?** - As in, where is it going to be stored?\n",
    "2. **How's it going to function?** - As in, does it return predictions immediatedly? or do they come later?\n",
    "\n",
    "![](09-deployment-questions-to-ask.png)\n",
    "\n",
    "*When starting to deploy machine learning models, it's helpful to start by asking what's the most ideal use case and then work backwards from there, asking where the model is going to go and then how it's going to function.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb68a47-1137-4fd0-8258-7db7884f96a6",
   "metadata": {},
   "source": [
    "### WHere's it going to go?\n",
    "\n",
    "When you deploy your machine learning model, where does it live?\n",
    "\n",
    "THe main debate here is usually on-device (also called edge/in the browser) or on the cloud (a computer/server that isn't the actual device someone/something calls the model from).\n",
    "\n",
    "Both have their pros and cons.\n",
    "\n",
    "| Deployment location| Pros| Cons|\n",
    "|:--|:--|:--|\n",
    "| **On-device(edge/in the browser** | Can be very fast (since no data leaves the device)|Limited compute power (larger models take longer to run)|\n",
    "| - | Privacy preserving (again no data has to leave the device)| Limited storage space (smaller model size required)|\n",
    "|-|No internet connection required (sometimes)|Device-specific skills often required|\n",
    "| **On cloud** | Near unlimited compute power (can scale up when needed)|Costs can get out of hand (if proper scaling limits aren't enforced)|\n",
    "|-|Can deploy one model and use everywhere (via API)|Predictions can be slower due to data having to leave device and predictions having to come back (network latency)|\n",
    "|-|Links into existing cloud ecosystem|Data has to leave device (this may cause privacy concerns)|\n",
    "\n",
    "There are more details to these but I've left resources in the extra-curriculum to learn more.\n",
    "\n",
    "Let's give an example.\n",
    "\n",
    "If we're deploying FoodVision as an app, we want it to perform well and fast.\n",
    "\n",
    "<div class='alert alert-success'>\n",
    "\n",
    "So which model would we prefer?\n",
    "1. A model on-device that performs at 95% accuracy with an inference time (latency) of one second per prediction.\n",
    "2. A model on the cloud that performs at 98% accuracy with an inference time of 10 seconds per prediction (bigger, better model but takes longer to compute).\n",
    "\n",
    "</div>\n",
    "\n",
    "We've made these numbers up but they showcase a potential difference between on-device and on the cloud.\n",
    "\n",
    "***Option 1** could potentially be a smaller less performant model that runs fast because its able to fit on a mobile device.*\n",
    "\n",
    "***Option 2** could potentially a larger more performant model that requires more compute and storage but it takes a bit longer to run because we have to send data off the device and get it back (so even though the actual predictoin might be fast, the network time and data transfer has to be factored in)\n",
    "\n",
    "**For FoodVision, we'd likely prefer 1, because the small hit in performance is outweighted by the faster inference speed.\n",
    "![](09-model-deployment-on-device-vs-cloud.png)\n",
    "\n",
    "*In the case of a Tesla car's computer vision system, which would be better? A smaller model that performs well on device (model is on the car) or a larger model that performs better that's on the cloud? In this case, you'd much prefer the model being on the car. The extra network time it would take for data to go from the car to the cloud and then back to the car just wouldn't be worth it (or potentially even impossible with poor signal areas).*\n",
    "\n",
    "***Note**: For a full example of seeing what it's like to deploy a PyTorch model to an edge device, see the PyTorch tutorial on achieving real-time inference (30fps+) with a computer vision model on a Raspberry Pi.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0633d6-e99f-475b-b472-9124e2b13c13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
