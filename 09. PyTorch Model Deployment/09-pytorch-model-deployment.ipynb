{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2958910d-be87-4adc-80b8-6c5bc7ecb2d0",
   "metadata": {},
   "source": [
    "# 09. PyTorch Model Deployment\n",
    "\n",
    "Lets bring FoodVision to life and make it publicly accessible.\n",
    "\n",
    "**We are going to deploy our FoodVision model to the internet as a usable app!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d5caad-ce3f-40bf-b4a0-d15ae0997135",
   "metadata": {},
   "source": [
    "## What is machine learning model deployment?\n",
    "\n",
    "***Machine learning model deployment** is the process of making your machine learning model accessible for others.*\n",
    "\n",
    "For example, someone taking a photo on their smartphone of food and then having our FoodVision model classify it into pizza, steak, or sushi.\n",
    "\n",
    "Some other examples can be, *an operating system may lower its resource consumption based on a machine learning model making predictions on how much power someone generally uses at specific times of day.*\n",
    "\n",
    "Also these models can learn from each other as well. For example, a Tesla car's computer vision system will interact with the car's route planning program and then the route planning program will get inputs and feedback from the driver.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069eaf5e-2a9f-4efa-a405-1f652a799a74",
   "metadata": {},
   "source": [
    "## Why deploy a machine learning model?\n",
    "\n",
    "One of the most important philosophical question in machine learning is:\n",
    "***if a machine learning model never leaves a notebook, does it exist?***\n",
    "\n",
    "---\n",
    "Deploying a model is as important as training one.\n",
    "\n",
    "Because although you can get a pretty good idea of how your model is going to function by evaluating it on a well crafted test set or visualizing its results, you never really know how it will perform ultil you release it to the wild.\n",
    "\n",
    "***Having people who've never used your model interact with it will often reveal edge cases you never thought of during training.***\n",
    "\n",
    "For example, what happens if someone was to upload a photo that wasn't of food to our FoodVision model?\n",
    "\n",
    "One solution would be to create another model that firstclassifies images as \"food\" or \"not food\" and passing the target image through that model first.\n",
    "\n",
    "THen if the images is of \"food\" it goes to our FoodVision model and gets classifies into pizza, steak, or sushi.\n",
    "\n",
    "And if it's \"not food\", a message is displayed.\n",
    "\n",
    "But what is these predictions were wrong?\n",
    "\n",
    "What happens then?\n",
    "\n",
    "You can see how these questions could keep going.\n",
    "\n",
    "Thus this highlights the importance of model deployment: it helps you figure our errors in your model that aren't obvious during training/testing.\n",
    "\n",
    "---\n",
    "\n",
    "***But once you've got a good model, deployment is a good next step. Monitoring involves seeing how your model goes on the most important data split: data from the real world.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a600cb-b1de-4d92-8e6e-b76d399c43b9",
   "metadata": {},
   "source": [
    "## Different types of machine learning model deployment\n",
    "\n",
    "Whole books could be written on the different types of machine learning model deployment(and many good one are listed [PyTorch Extra Resources](https://www.learnpytorch.io/pytorch_extra_resources/#resources-for-machine-learning-and-deep-learning-engineering))\n",
    "\n",
    "---\n",
    "Let's start with a simple question:\n",
    "> What is the most ideal scenario for our machine leanring model to be used?\n",
    "\n",
    "ANd then work backward from there.\n",
    "\n",
    "In case of FoodVision, our ideal scenerio might be:\n",
    "- someone taking a photo on a mobile device(through an app or web browser)\n",
    "- The prediction comes back fast.\n",
    "\n",
    "Easy.\n",
    "\n",
    "So we have two main criteria:\n",
    "1. The model should work on a mobile device\n",
    "2. THe model should make predictions *fast*(because a slow app is a boring app).\n",
    "\n",
    "And of course, depending on our use case, our requirements may vary.\n",
    "\n",
    "We may notive the above two points break down into another two questions:\n",
    "1. **Where's it going to go?** - As in, where is it going to be stored?\n",
    "2. **How's it going to function?** - As in, does it return predictions immediatedly? or do they come later?\n",
    "\n",
    "![](09-deployment-questions-to-ask.png)\n",
    "\n",
    "*When starting to deploy machine learning models, it's helpful to start by asking what's the most ideal use case and then work backwards from there, asking where the model is going to go and then how it's going to function.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb68a47-1137-4fd0-8258-7db7884f96a6",
   "metadata": {},
   "source": [
    "## Where's it going to go?\n",
    "\n",
    "When you deploy your machine learning model, where does it live?\n",
    "\n",
    "THe main debate here is usually on-device (also called edge/in the browser) or on the cloud (a computer/server that isn't the actual device someone/something calls the model from).\n",
    "\n",
    "Both have their pros and cons.\n",
    "\n",
    "| Deployment location| Pros| Cons|\n",
    "|:--|:--|:--|\n",
    "| **On-device(edge/in the browser** | Can be very fast (since no data leaves the device)|Limited compute power (larger models take longer to run)|\n",
    "| - | Privacy preserving (again no data has to leave the device)| Limited storage space (smaller model size required)|\n",
    "|-|No internet connection required (sometimes)|Device-specific skills often required|\n",
    "| **On cloud** | Near unlimited compute power (can scale up when needed)|Costs can get out of hand (if proper scaling limits aren't enforced)|\n",
    "|-|Can deploy one model and use everywhere (via API)|Predictions can be slower due to data having to leave device and predictions having to come back (network latency)|\n",
    "|-|Links into existing cloud ecosystem|Data has to leave device (this may cause privacy concerns)|\n",
    "\n",
    "There are more details to these but I've left resources in the extra-curriculum to learn more.\n",
    "\n",
    "Let's give an example.\n",
    "\n",
    "If we're deploying FoodVision as an app, we want it to perform well and fast.\n",
    "\n",
    "<div class='alert alert-success'>\n",
    "\n",
    "So which model would we prefer?\n",
    "1. A model on-device that performs at 95% accuracy with an inference time (latency) of one second per prediction.\n",
    "2. A model on the cloud that performs at 98% accuracy with an inference time of 10 seconds per prediction (bigger, better model but takes longer to compute).\n",
    "\n",
    "</div>\n",
    "\n",
    "We've made these numbers up but they showcase a potential difference between on-device and on the cloud.\n",
    "\n",
    "***Option 1** could potentially be a smaller less performant model that runs fast because its able to fit on a mobile device.*\n",
    "\n",
    "***Option 2** could potentially a larger more performant model that requires more compute and storage but it takes a bit longer to run because we have to send data off the device and get it back (so even though the actual predictoin might be fast, the network time and data transfer has to be factored in)\n",
    "\n",
    "**For FoodVision, we'd likely prefer 1, because the small hit in performance is outweighted by the faster inference speed.\n",
    "![](09-model-deployment-on-device-vs-cloud.png)\n",
    "\n",
    "*In the case of a Tesla car's computer vision system, which would be better? A smaller model that performs well on device (model is on the car) or a larger model that performs better that's on the cloud? In this case, you'd much prefer the model being on the car. The extra network time it would take for data to go from the car to the cloud and then back to the car just wouldn't be worth it (or potentially even impossible with poor signal areas).*\n",
    "\n",
    "***Note**: For a full example of seeing what it's like to deploy a PyTorch model to an edge device, see the PyTorch tutorial on achieving real-time inference (30fps+) with a computer vision model on a Raspberry Pi.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83e3000-43f2-4486-adc3-536338005860",
   "metadata": {},
   "source": [
    "## How's it going to function?\n",
    "\n",
    "Back to the ideal use case, when you deploy your machine learning, **How should it work?**\n",
    "\n",
    "1. *As in, would you like predictions returned immediately??*\n",
    "\n",
    "2. *or is it okay for them to happen later?*\n",
    "\n",
    "These two scenarios are generally referred to as:\n",
    "- Online (real-time) - *Predictions/inference happen **immediately***. For eample, someone uploads an image, the image gets transformed and predictions are returned or someone makes a purchase and the transaction is verified to be non-fraudulent by a model so the purchase can go through.\n",
    "- Offline (batch) - *Predictions/inference happen **periodically***. For example, a photos application sorts your images into different categories (such as beach, mealtime, family, friends) whilst your mobile device is plugged into charge.\n",
    "\n",
    "***Note:** `Batch` refers to inference being performed on multiple samples at a time. However, to add a little confusion, batch processing can happen immediately/online (multiple images being classified at once) and/or offline (multiple images being predicted/trained on at once).*\n",
    "\n",
    "The main difference between each being: prediction being made immediately or periodically.\n",
    "\n",
    "Periodically can have a varying timescale too, from every few seconds to every few hours or days.\n",
    "\n",
    "And you can mix and match the two.\n",
    "\n",
    "In the case of FoodVision, we'd want our inference pipeline to happen online (real-time), so when someone uploads an image of pizza, steak, or sushi, the prediction results are returned immediately (any slower than real-time would make a boring experience).\n",
    "\n",
    "But for our training pipeline, it's okay for it to happen in a batch (offline) fashion, which is what we've beein doing throughout the previous chapters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ee69bf-01d4-447a-9cf6-6242e73f0136",
   "metadata": {},
   "source": [
    "## Ways to deploy a machine learning model\n",
    "\n",
    "We've discussed a couple of options for deploying machine learning models (on-device and cloud)\n",
    "\n",
    "And each of these will have their specific requirements.\n",
    "\n",
    "| Tool/Resource | Deployment Type|\n",
    "|:-|:-|\n",
    "| [Google's ML Kit](https://developers.google.com/ml-kit)| On-device (Android and iOS)|\n",
    "| [Apple's Core ML](https://developer.apple.com/documentation/coreml) and [`coremltools` Python package](https://apple.github.io/coremltools/docs-guides/) | On-device (all Apple devices)|\n",
    "| [Amazon Web Service's (AWS) Sagemaker](https://aws.amazon.com/sagemaker/)| Cloud|\n",
    "| [Google Cloud's Vertex AI](https://cloud.google.com/vertex-ai) | Cloud |\n",
    "| [Microsoft's Azure Machine Learning](https://azure.microsoft.com/en-au/services/machine-learning/) | Cloud|\n",
    "| [HuggingFace Spaces](https://huggingface.co/spaces) | Cloud |\n",
    "| API with [FastAPI](https://fastapi.tiangolo.com/) | Cloud/self-hosted server |\n",
    "|API with [TorchServe](https://pytorch.org/serve/) | Cloud/self-hosted server |\n",
    "| [ONNX (open Neural Network Exchange)](https://pytorch.org/serve/) | Many/general|\n",
    "| Many more..|-|\n",
    "\n",
    "***Note**: An application programming interface (API) is a way for two (or more) computer programs to interact with each other. For example, if your model was deployed as API, you would be able to write a program that could send data to it and then receive predictions back.*\n",
    "\n",
    "Which option you choose will be highly dependent on what you're building/who you're working with.\n",
    "\n",
    "But with so many options, it can be very intimidating.\n",
    "\n",
    "So best to start small and keep it simple.\n",
    "\n",
    "And one of the best ways to do so is by turning your machine learning model into a demp app with [***Gradio***](https://gradio.app/) and then deploying it on Hugging Face spaces.\n",
    "\n",
    "We'll be doing just that with FoodVision later on.\n",
    "![](09-tools-and-places-to-deploy-ml-models.png)\n",
    "\n",
    "*A handful of places and tools to host and deploy machine learning models. There are plenty we've missed, feel free to find them later.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d55f3c6-15b3-44c9-8739-5fe0ff5f253e",
   "metadata": {},
   "source": [
    "## What we're going to cover\n",
    "\n",
    "Let's become a machine learning engineer and actually deploy a machine learning model.\n",
    "\n",
    "Our goal is to deploy our FoodVision Model via a demo Gradio app with the following metrics:\n",
    "1. **Performance**: 95% accuracy.\n",
    "2. **Speed**: real-time inference of 30FPS+(each prediction has a latency of lower than ~0.03s)\n",
    "\n",
    "We'll start by running an experiment to compare our best two models so far: EffNetB2 and ViT feature extractors.\n",
    "\n",
    "Then we'll deploy the one which performs closest to our goal metrics.\n",
    "\n",
    "***Finally, we'll finish with a (BIG) surprise bonus.***\n",
    "\n",
    "| Topic |\n",
    "|:-|\n",
    "|**0. Setting up** |\n",
    "|**1. Get data** |\n",
    "|**2. FoodVision Mini model deployment experiment outline**|\n",
    "|**3. Creating an EffNetB2 feature extractor**|\n",
    "|**4. Creating a ViT feature Extractor**|\n",
    "|**5. Making predictions with our trained models and timing them**|\n",
    "|**6. Comparing model results, prediction times and size**|\n",
    "|**7. Bringing FoodVision to life by creating a Gradio demo**|\n",
    "|**8. Turning our FoodVision Gradio demo into a deployable app**|\n",
    "|**9. Deploying our gradio demo to HuggingFace Spaces**|\n",
    "|**10. Creating a BiG surprise**|\n",
    "|**11. Deploying our BiG suprise****|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fc76aa-bf69-4c1c-a046-45fe1d18a54d",
   "metadata": {},
   "source": [
    "## 0. Setting up\n",
    "\n",
    "As we've dome previously, let's make sure we've got all of the modules we'll need for this section.\n",
    "\n",
    "We'll import the Python scripts (such as `data_setup.py` and `engine.py`) we created in 05. PyTorch Going Modular.\n",
    "\n",
    "To do so, we'll download `going_modular` directory from the `pytorch-deep-learning` repository (if we dont already have it).\n",
    "\n",
    "We'll also get the `torchinfo` package if it's not available.\n",
    "\n",
    "`torchinfo` will help later on to give us visual representation of our model.\n",
    "\n",
    "ANd since later on we'll be using `torchvision`, we'll make sure we've got the latest versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7110c91-9e03-41cf-8360-fa62310b8387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.2.2\n",
      "torchvision version: 0.17.2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "print(f\"torch version: {torch.__version__}\")\n",
    "print(f\"torchvision version: {torchvision.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387e5df6-9809-4d7e-908b-f7d47845a531",
   "metadata": {},
   "source": [
    "Now we'll continue with the regular imports, setting up device agnostic code, and this time we'll also get the `helper_functions.py` script from GitHub.\n",
    "\n",
    "The `helper_functions.py` scrip contains several functions we created in previous sections:\n",
    "- `set_seeds()` - sets the random seeds\n",
    "- `download_data()` - to download a data source given a link\n",
    "- `plot_loss_curves()` - to inspect our model's training results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "572bc5ae-5973-4ec4-bc2f-179dd5264de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Couldn't find going_modular or helper_functions scripts... downloading them from GitHub.\n",
      "Cloning into 'pytorch-deep-learning'...\n",
      "remote: Enumerating objects: 4356, done.\u001b[K\n",
      "remote: Counting objects: 100% (321/321), done.\u001b[K\n",
      "remote: Compressing objects: 100% (143/143), done.\u001b[K\n",
      "remote: Total 4356 (delta 213), reused 255 (delta 177), pack-reused 4035 (from 1)\u001b[K\n",
      "Receiving objects: 100% (4356/4356), 654.51 MiB | 22.92 MiB/s, done.\n",
      "Resolving deltas: 100% (2584/2584), done.\n",
      "Updating files: 100% (248/248), done.\n"
     ]
    }
   ],
   "source": [
    "# Continue with regular imports\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from torchinfo import summary\n",
    "\n",
    "# Try to import the going_modular directory, download it from GitHub if it doesn't work\n",
    "try:\n",
    "    from going_modular.going_modular import data_setup, engine\n",
    "    from helper_functions import download_data, set_seeds, plot_loss_curves\n",
    "except:\n",
    "    # Get the going_modular scripts\n",
    "    print(\"[INFO] Couldn't find going_modular or helper_functions scripts... downloading them from GitHub.\")\n",
    "    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n",
    "    !mv pytorch-deep-learning/going_modular .\n",
    "    !mv pytorch-deep-learning/helper_functions.py . # get the helper_functions.py script\n",
    "    !rm -rf pytorch-deep-learning\n",
    "    from going_modular.going_modular import data_setup, engine\n",
    "    from helper_functions import download_data, set_seeds, plot_loss_curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a266e365-f871-4da3-81e3-f7ed2c315af9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# device agnostic code\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b75bee7-bf8b-4715-b598-6be91157ed58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
